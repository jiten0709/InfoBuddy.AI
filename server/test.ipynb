{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6773bc6a",
   "metadata": {},
   "source": [
    "# Comprehensive test suite for the InfoBuddy.AI Server\n",
    "\n",
    "This notebook provides a comprehensive implementation of a chat assistant using:\n",
    "- <xyz> llm for conversational AI\n",
    "- Tavily Search for real-time web information\n",
    "- LangGraph for workflow orchestration\n",
    "- Memory management for conversation continuity\n",
    "\n",
    "Features:\n",
    "- Class-based architecture with clear separation of concerns\n",
    "- Comprehensive error handling and logging\n",
    "- Type safety with complete annotations\n",
    "- Debugging and visualization capabilities\n",
    "- Industry best practices implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ff7040",
   "metadata": {},
   "source": [
    "## dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d4f44c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Optional, TypedDict, Any, Union, Annotated, Dict\n",
    "from langgraph.graph import add_messages, StateGraph, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from uuid import uuid4\n",
    "\n",
    "import os\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "import sys\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler(filename='logs/test.log', mode='a')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9715107c",
   "metadata": {},
   "source": [
    "## schema / state definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22663e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatConversationState(TypedDict):\n",
    "    \"\"\"\n",
    "    Enhanced state definition for chat conversation management.\n",
    "    \n",
    "    This TypedDict defines the structure of the conversation state that flows\n",
    "    through the LangGraph workflow, ensuring type safety and clear data contracts.\n",
    "    \n",
    "    Attributes:\n",
    "        messages: List of conversation messages with automatic message addition functionality\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce91357b",
   "metadata": {},
   "source": [
    "## chat assistant model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1deb3e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatAssistantNotebook:\n",
    "    \"\"\"\n",
    "    Production-ready chat assistant implementation for Jupyter notebooks.\n",
    "    \n",
    "    This class encapsulates all chat assistant functionality including:\n",
    "    - LLM integration with <xyz> llm\n",
    "    - Web search capabilities via Tavily\n",
    "    - Conversation memory management\n",
    "    - Graph-based workflow orchestration\n",
    "    - Comprehensive error handling and logging\n",
    "    \n",
    "    Attributes:\n",
    "        model: <xyz> model instance\n",
    "        search_tool: Tavily search tool for web queries\n",
    "        memory_manager: Conversation memory checkpoint manager\n",
    "        conversation_graph: Compiled LangGraph workflow\n",
    "        \n",
    "    Example:\n",
    "        >>> assistant = ChatAssistantNotebook()\n",
    "        >>> await assistant.process_user_message(\"What's the weather today?\")\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str = 'gemini-2.5-flash',\n",
    "            max_search_results: int = 2,\n",
    "    ):\n",
    "        logger.info(\"🚀 Initializing Enhanced Chat Assistant...\")\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.max_search_results = max_search_results\n",
    "        self.app = None\n",
    "        self.model = None\n",
    "        self.search_tool = None\n",
    "        self.memory = None\n",
    "        self.available_tools = []\n",
    "        self.llm = None\n",
    "        self.enhanced_llm = None\n",
    "\n",
    "        try:\n",
    "            \n",
    "            self._initialize_language_model()\n",
    "            self._initialize_search_capabilities()\n",
    "            self._initialize_memory_management()\n",
    "            self._setup_conversation_workflow()\n",
    "            logger.info(\"✅ Chat Assistant initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Initialization failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _initialize_language_model(self) -> None:\n",
    "        \"\"\" Initialize the <xyz> language model\"\"\"\n",
    "        try:\n",
    "            self.llm = ChatGoogleGenerativeAI(model=self.model_name, temperature=0.1)\n",
    "            logger.info(f\"✅ Language model '{self.model_name}' initialized.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to initialize language model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _initialize_search_capabilities(self) -> None:\n",
    "        \"\"\" Initialize the Tavily search tool \"\"\"\n",
    "        try:\n",
    "            self.search_tool = TavilySearchResults(max_results=self.max_search_results)\n",
    "            self.available_tools = [self.search_tool]\n",
    "\n",
    "            # bind tools to llm\n",
    "            self.enhanced_llm = self.llm.bind_tools(tools=self.available_tools)\n",
    "\n",
    "            logger.info(\"✅ Search tool initialized and enhances llm capabilities.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to initialize search tool capabilities: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _initialize_memory_management(self) -> None:\n",
    "        \"\"\" Initialize the conversation memory manager\"\"\"\n",
    "        try: \n",
    "            self.memory = MemorySaver()\n",
    "            logger.info(\"✅ Memory manager initialized.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to initialize memory manager: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _setup_conversation_workflow(self) -> None:\n",
    "        \"\"\" Setup the LangGraph conversation workflow\"\"\"\n",
    "        try:\n",
    "            # create the state graph\n",
    "            graph = StateGraph(ChatConversationState)\n",
    "\n",
    "            # add nodes\n",
    "            graph.add_node('llm_node', self._execute_language_model_processing)\n",
    "            graph.add_node('tool_execution_node', self._execute_tool_operations)\n",
    "\n",
    "            # set entry point\n",
    "            graph.set_entry_point('llm_node')\n",
    "\n",
    "            # add edges\n",
    "            graph.add_conditional_edges(\n",
    "                'llm_node', \n",
    "                self._route_to_appropriate_node,\n",
    "                {\n",
    "                    'tool_execution_node': 'tool_execution_node',\n",
    "                    END: END\n",
    "                }\n",
    "            )\n",
    "            graph.add_edge('tool_execution_node', 'llm_node')\n",
    "\n",
    "            # compile the graph\n",
    "            self.app = graph.compile(checkpointer=self.memory)\n",
    "\n",
    "            logger.info(\"✅ Conversation workflow graph created and compiled.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to create state graph: {e}\")\n",
    "            raise\n",
    "\n",
    "    async def _execute_language_model_processing(self, state: ChatConversationState) -> Dict[str, List[BaseMessage]]:\n",
    "        \"\"\" Execute the language model processing step\"\"\"\n",
    "        try:\n",
    "            logger.debug(\"🧠 Processing messages through language model...\")\n",
    "            res = await self.enhanced_llm.ainvoke(state['messages'])\n",
    "            logger.debug(f\"🔍 Model response: {res}\")\n",
    "            return {'messages': res}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ LLM processing failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    async def _route_to_appropriate_node(self, state: ChatConversationState) -> str:\n",
    "        \"\"\" Determine the next node in the workflow\"\"\"\n",
    "        try:\n",
    "            last_msg = state['messages'][-1]\n",
    "\n",
    "            # check if last message contains a tool call\n",
    "            has_tool_calls = (\n",
    "                hasattr(last_msg, 'tool_calls') and\n",
    "                len(last_msg.tool_calls) > 0\n",
    "            )\n",
    "            if has_tool_calls:\n",
    "                logger.info(f\"🔧 Tool execution required: {len(last_msg.tool_calls)} calls\")\n",
    "                return 'tool_execution_node'\n",
    "            else:\n",
    "                logger.debug(\"✅ No tool calls detected, ending workflow.\")\n",
    "                return END\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Node routing failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    async def _execute_tool_operations(self, state: ChatConversationState) -> Dict[str, List[ToolMessage]]:\n",
    "        \"\"\" Execute any required tool operations\"\"\"\n",
    "        try:\n",
    "            logger.info(\"🔧 Executing tool operations...\")\n",
    "            tool_calls = state['messages'][-1].tool_calls\n",
    "            executed_tools = []\n",
    "\n",
    "            for t in tool_calls:\n",
    "                logger.debug(f\"🔍 Executing tool: {t['name']} with input: {t['args']}\")\n",
    "                tool_result = await self._process_individual_tool_call(t)\n",
    "                executed_tools.append(tool_result)\n",
    "            \n",
    "            logger.debug(f\"✅ Executed {len(executed_tools)} & its results: {executed_tools}\")\n",
    "            return {'messages': executed_tools}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Tool execution failed: {e}\")\n",
    "            return {'messages': []}\n",
    "        \n",
    "    async def _process_individual_tool_call(self, tool_call: Dict[str, Any]) -> ToolMessage:\n",
    "        \"\"\" Process an individual tool call\"\"\"\n",
    "        tool_name = tool_call['name']\n",
    "        tool_args = tool_call.get('args', {})\n",
    "        tool_identifier = tool_call['id']\n",
    "\n",
    "        logger.debug(f\"🔍 Processing tool call id: {tool_identifier} || name: {tool_name} || with args: {tool_args}\")\n",
    "        try:\n",
    "            if tool_name == self.search_tool.name:\n",
    "                search_results = await self.search_tool.ainvoke(tool_args)\n",
    "                tool_msg = ToolMessage(\n",
    "                    content=str(search_results),\n",
    "                    tool_call_id=tool_identifier,\n",
    "                    name=tool_name\n",
    "                )\n",
    "                logger.info(f\"🔍 Search completed for query: {tool_args.get('query', 'unknown')}\")\n",
    "                return tool_msg \n",
    "            else:\n",
    "                logger.warning(f\"⚠️ Unknown tool requested: {tool_name}\")\n",
    "                return ToolMessage(\n",
    "                    content=f\"Unknown tool: {tool_name}\",\n",
    "                    tool_call_id=tool_identifier,\n",
    "                    name=tool_name\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Tool {tool_name} execution failed: {str(e)}\")\n",
    "            return ToolMessage(\n",
    "                content=f\"Tool execution failed: {str(e)}\",\n",
    "                tool_call_id=tool_identifier,\n",
    "                name=tool_name\n",
    "            )\n",
    "        \n",
    "    def visualize_conversation_workflow(self) -> None:\n",
    "        \"\"\" Visualize the conversation workflow graph\"\"\"\n",
    "        try:\n",
    "            graph_obj_getter = getattr(self.app, 'get_graph', None)\n",
    "            graph_obj = graph_obj_getter() if callable(graph_obj_getter) else self._graph\n",
    "\n",
    "            # 2. Mermaid source ---------------------------------------------------\n",
    "            if hasattr(graph_obj, 'draw_mermaid'):\n",
    "                try:\n",
    "                    mermaid_src = graph_obj.draw_mermaid()\n",
    "                    print(mermaid_src)\n",
    "                    logger.info(\"🧪 Mermaid diagram (text) printed.\")\n",
    "                except Exception as e:\n",
    "                    logger.debug(\"⚠️ Mermaid render not available: %s\", e)\n",
    "\n",
    "            # 3. ASCII fallback ---------------------------------------------------\n",
    "            if hasattr(graph_obj, 'draw_ascii'):\n",
    "                try:\n",
    "                    ascii_map = graph_obj.draw_ascii()\n",
    "                    print(ascii_map)\n",
    "                    logger.info(\"📄 ASCII graph printed.\")\n",
    "                    return\n",
    "                except Exception as e:\n",
    "                    logger.debug(\"⚠️ ASCII render not available: %s\", e)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"❌ Graph visualization failed: {e}\")\n",
    "\n",
    "    async def process_user_message(\n",
    "            self,\n",
    "            user_message: str,\n",
    "            conversation_id: Optional[Union[str, int]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Process a user message through the complete conversation workflow.\n",
    "        \n",
    "        Args:\n",
    "            user_message: The message from the user to process\n",
    "            conversation_id: Optional conversation identifier for memory continuity\n",
    "        \"\"\"\n",
    "        if not user_message or not user_message.strip():\n",
    "            logger.error(\"⚠️ Empty user message provided, skipping processing.\")\n",
    "            raise ValueError(\"User message cannot be empty.\")\n",
    "        \n",
    "        # generate a conversation id if not provided\n",
    "        if conversation_id is None:\n",
    "            conversation_id = str(uuid4())\n",
    "            logger.debug(f\"🆕 Generated new conversation ID: {conversation_id}\")\n",
    "        \n",
    "        conversation_config = {\n",
    "            'configurable': {\n",
    "                'thread_id': conversation_id\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(f\"\\n👤 User: {user_message}\")\n",
    "        logger.info(f\"👤 User (id: {conversation_id}): {user_message}\")\n",
    "        print(\"🤖 Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "        assistant_response = \"\"\n",
    "        search_performed = False\n",
    "\n",
    "        try:\n",
    "            async for e in self.app.astream(\n",
    "                {'messages': [HumanMessage(content=user_message)]},\n",
    "                config=conversation_config,\n",
    "                version='v2'\n",
    "            ):\n",
    "                event_type = e.get('event', '')\n",
    "                if event_type == 'on_chat_model_stream':\n",
    "                    chunk = e.get('data', {}).get('chunk')\n",
    "                    if chunk and hasattr(chunk, 'content'):\n",
    "                        chunk_content = getattr(chunk, 'content', '')\n",
    "                        if chunk_content:\n",
    "                            print(chunk.content, end=\"\", flush=True)\n",
    "                            logger.debug(f\"💬 Streaming: {chunk.content}\")\n",
    "                            assistant_response += chunk.content\n",
    "                elif event_type == 'on_tool_start':\n",
    "                    tool_name = e.get('data', {}).get('tool_name', '')\n",
    "                    if tool_name == \"tavily_search_results_json\":\n",
    "                        if not search_performed:\n",
    "                            print(\"\\n🔍 Searching the web...\", end=\"\", flush=True)\n",
    "                            logger.debug(f\"🔧 Tool starting: {tool_name}\")\n",
    "                            search_performed = True\n",
    "                elif event_type == \"on_tool_end\":\n",
    "                    tool_name = e.get(\"name\", \"\")\n",
    "                    if tool_name == \"tavily_search_results_json\":\n",
    "                        print(\"\\n🤖 Assistant: \", end=\"\", flush=True)\n",
    "                        logger.debug(f\"✅ Tool completed: {tool_name}\")\n",
    "                print()  \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Message processing failed: {e}\")\n",
    "            print(f\"\\n❌ An error occurred: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa269843",
   "metadata": {},
   "source": [
    "## Run demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a852db",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demonstrate_chat_assistant():\n",
    "    \"\"\"\n",
    "    Demonstration function showing how to use the enhanced chat assistant.\n",
    "    \n",
    "    This function provides examples of:\n",
    "    - Basic initialization\n",
    "    - Interactive chat display\n",
    "    - Message processing with real-time streaming\n",
    "    - Error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"🎯 Starting Chat Assistant Demonstration...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Initialize the chat assistant\n",
    "        chat_assistant = ChatAssistantNotebook(\n",
    "            model_name=\"gemini-2.5-flash\",\n",
    "            max_search_results=2,\n",
    "        )\n",
    "\n",
    "        chat_assistant.visualize_conversation_workflow()\n",
    "        \n",
    "        print(\"✅ Chat Assistant initialized successfully!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Process example messages\n",
    "        test_messages = [\n",
    "            \"Hi, my name is Jiten\",\n",
    "            \"When is the next SpaceX launch?\",\n",
    "            \"Whats my name?\"\n",
    "        ]\n",
    "        \n",
    "        for message in test_messages:\n",
    "            await chat_assistant.process_user_message(message)\n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        print(\"✅ Demonstration completed successfully\")\n",
    "        \n",
    "    except Exception as demo_error:\n",
    "        print(f\"❌ Demonstration failed: {str(demo_error)}\")\n",
    "        logger.error(f\"❌ Demonstration failed: {str(demo_error)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b2798eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting Chat Assistant Demonstration...\n",
      "============================================================\n",
      "2025-09-06 15:22:40 | INFO     | __main__ | 🚀 Initializing Enhanced Chat Assistant...\n",
      "2025-09-06 15:22:40 | INFO     | __main__ | ✅ Language model 'gemini-2.5-flash' initialized.\n",
      "2025-09-06 15:22:40 | INFO     | __main__ | ✅ Search tool initialized and enhances llm capabilities.\n",
      "2025-09-06 15:22:40 | INFO     | __main__ | ✅ Memory manager initialized.\n",
      "2025-09-06 15:22:40 | INFO     | __main__ | ✅ Conversation workflow graph created and compiled.\n",
      "2025-09-06 15:22:40 | INFO     | __main__ | ✅ Chat Assistant initialized successfully.\n",
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tllm_node(llm_node)\n",
      "\ttool_execution_node(tool_execution_node)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> llm_node;\n",
      "\tllm_node -.-> __end__;\n",
      "\tllm_node -.-> tool_execution_node;\n",
      "\ttool_execution_node --> llm_node;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n",
      "2025-09-06 15:22:40 | INFO     | __main__ | 🧪 Mermaid diagram (text) printed.\n",
      "             +-----------+                     \n",
      "             | __start__ |                     \n",
      "             +-----------+                     \n",
      "                    *                          \n",
      "                    *                          \n",
      "                    *                          \n",
      "              +----------+                     \n",
      "              | llm_node |                     \n",
      "              +----------+                     \n",
      "            ..            **                   \n",
      "          ..                **                 \n",
      "        ..                    **               \n",
      "+---------+           +---------------------+  \n",
      "| __end__ |           | tool_execution_node |  \n",
      "+---------+           +---------------------+  \n",
      "2025-09-06 15:22:40 | INFO     | __main__ | 📄 ASCII graph printed.\n"
     ]
    }
   ],
   "source": [
    "await demonstrate_chat_assistant()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d362aaf",
   "metadata": {},
   "source": [
    "## Run interactive chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65f7c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def interactive_chat():\n",
    "    \"\"\"\n",
    "    Interactive chat function for manual testing with user input.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"🚀 Starting Interactive Chat Assistant...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Initialize the chat assistant\n",
    "        chat_assistant = ChatAssistantNotebook(\n",
    "            model_name=\"gemini-2.5-flash\",\n",
    "            max_search_results=4,\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Chat Assistant ready! Type 'quit' to exit.\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        conversation_id = uuid4()\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\n👤 You: \").strip()\n",
    "                \n",
    "                if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                    print(\"👋 Goodbye!\")\n",
    "                    break\n",
    "                \n",
    "                if not user_input:\n",
    "                    print(\"⚠️ Please enter a message.\")\n",
    "                    continue\n",
    "                \n",
    "                await chat_assistant.process_user_message(user_input, conversation_id)\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n👋 Chat interrupted. Goodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error: {str(e)}\")\n",
    "                \n",
    "    except Exception as init_error:\n",
    "        print(f\"❌ Failed to initialize chat: {str(init_error)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b05506e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Interactive Chat Assistant...\n",
      "============================================================\n",
      "2025-09-06 14:24:52 | INFO     | __main__ | 🚀 Initializing Enhanced Chat Assistant...\n",
      "2025-09-06 14:24:52 | INFO     | __main__ | ✅ Language model 'gemini-2.5-flash' initialized.\n",
      "2025-09-06 14:24:52 | INFO     | __main__ | ✅ Search tool initialized and enhances llm capabilities.\n",
      "2025-09-06 14:24:52 | INFO     | __main__ | ✅ Memory manager initialized.\n",
      "2025-09-06 14:24:52 | INFO     | __main__ | ✅ Conversation workflow graph created and compiled.\n",
      "2025-09-06 14:24:52 | INFO     | __main__ | ✅ Chat Assistant initialized successfully.\n",
      "✅ Chat Assistant ready! Type 'quit' to exit.\n",
      "============================================================\n",
      "\n",
      "👤 User: hi my name is jiten\n",
      "2025-09-06 14:25:00 | INFO     | __main__ | 👤 User (id: 285829db-1aaf-4470-85f6-902094fcfa66): hi my name is jiten\n",
      "🤖 Assistant: 2025-09-06 14:25:00 | DEBUG    | __main__ | 🧠 Processing messages through language model...\n",
      "2025-09-06 14:25:02 | DEBUG    | __main__ | 🔍 Model response: content='Hi Jiten, nice to meet you! How can I help you today?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--0a318889-e4c1-4b6c-adeb-0bd36d22f3a2-0' usage_metadata={'input_tokens': 80, 'output_tokens': 16, 'total_tokens': 96, 'input_token_details': {'cache_read': 0}}\n",
      "2025-09-06 14:25:02 | DEBUG    | __main__ | ✅ No tool calls detected, ending workflow.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "👤 User: hows the weather in mumbai today?\n",
      "2025-09-06 14:25:24 | INFO     | __main__ | 👤 User (id: 285829db-1aaf-4470-85f6-902094fcfa66): hows the weather in mumbai today?\n",
      "🤖 Assistant: 2025-09-06 14:25:24 | DEBUG    | __main__ | 🧠 Processing messages through language model...\n",
      "2025-09-06 14:25:25 | DEBUG    | __main__ | 🔍 Model response: content='' additional_kwargs={'function_call': {'name': 'tavily_search_results_json', 'arguments': '{\"query\": \"weather in Mumbai today\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--f37f069d-2ad3-4e9f-9c79-49cdccbcf981-0' tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in Mumbai today'}, 'id': '33c921b1-61be-4736-86fd-2da090a33f2b', 'type': 'tool_call'}] usage_metadata={'input_tokens': 106, 'output_tokens': 78, 'total_tokens': 184, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 54}}\n",
      "2025-09-06 14:25:25 | INFO     | __main__ | 🔧 Tool execution required: 1 calls\n",
      "\n",
      "2025-09-06 14:25:25 | INFO     | __main__ | 🔧 Executing tool operations...\n",
      "2025-09-06 14:25:25 | DEBUG    | __main__ | 🔍 Executing tool: tavily_search_results_json with input: {'query': 'weather in Mumbai today'}\n",
      "2025-09-06 14:25:25 | DEBUG    | __main__ | 🔍 Processing tool call id: 33c921b1-61be-4736-86fd-2da090a33f2b || name: tavily_search_results_json || with args: {'query': 'weather in Mumbai today'}\n",
      "2025-09-06 14:25:31 | INFO     | __main__ | 🔍 Search completed for query: weather in Mumbai today\n",
      "2025-09-06 14:25:31 | DEBUG    | __main__ | ✅ Executed 1 & its results: [ToolMessage(content='[{\\'title\\': \\'Mumbai weather in September 2025 | India: How hot?\\', \\'url\\': \\'https://www.weather2travel.com/india/mumbai/september/\\', \\'content\\': \"weather2travel.com - travel deals for your holiday in the sun\\\\nClick to search\\\\n\\\\n# Mumbai weather in September 2025\\\\n\\\\nExpect  daytime maximum temperatures of 30°C in Mumbai, India in September and high heat and humidity based on long-term weather averages. There are 6 hours of sunshine per day on average with 13 days with some rainfall and typically 347 mm of rainfall in the month. [...] 30°C maximum daytime temperature in September in Mumbai\\\\n6 hours of sunshine per day (47% of daylight hours) in September in Mumbai\\\\n13 days with some rainfall in September in Mumbai\\\\n25°C minimum night-time temperature in September in Mumbai\\\\n12 hours of daylight per day in September in Mumbai\\\\nHigh heat & humidity in September in Mumbai\\\\n347 mm monthly rainfall in September in Mumbai\\\\nUV (maximum) index 11+ (Extreme) in September in Mumbai\\\\n28°C sea temperature in September in Mumbai [...] 9 must-see sights for first-time travellers to India\\\\n9 must-see sights for first-time travellers to India\\\\n6 misconceptions about New Delhi\\\\n6 misconceptions about New Delhi\\\\n9 reasons to visit Goa (which don\\'t involve beaches)\\\\n9 reasons to visit Goa (which don\\'t involve beaches)\\\\n\\\\n### How hot is it in Mumbai in September?\\\\n\\\\nDaytime temperatures usually reach 30°C in Mumbai in September with high heat and humidity, falling to 25°C at night.\\\\n\\\\n### How sunny is it in Mumbai in September?\", \\'score\\': 0.8430832}, {\\'title\\': \\'Mumbai weather in September 2025 - Weather25.com\\', \\'url\\': \\'https://www.weather25.com/asia/india/maharashtra/mumbai?page=month&month=September\\', \\'content\\': \\'The weather in Mumbai in September is very hot. The average temperatures are between 26°C and 30°C, drinking water regularly is advisable.\\\\n\\\\nYou can expect rain for roughly half of the month of September in Mumbai. We’re expecting roughly 8 to 15 days of rain, so your rubber boots and umbrella are going to see plenty of use this month if you’re keen on staying dry.\\\\n\\\\nOur weather forecast can give you a great sense of what weather to expect in Mumbai in September 2025. [...] weather25.com\\\\nSearch\\\\nweather in India\\\\nRemove from your favorite locations\\\\nAdd to my locations\\\\nShare\\\\nweather in India\\\\n\\\\n# Mumbai weather in September 2025\\\\n\\\\nLight rain shower\\\\nLight rain shower\\\\nLight rain shower\\\\nPatchy rain possible\\\\nPatchy rain possible\\\\nPatchy rain possible\\\\nCloudy\\\\nPatchy rain possible\\\\nPatchy rain possible\\\\nModerate or heavy rain shower\\\\nLight rain shower\\\\nLight rain shower\\\\nLight rain shower\\\\nLight rain shower\\\\n\\\\n## The average weather in Mumbai in September [...] | June | 31° / 28° | 17 | 13 | 0 | 673 mm | Ok | Mumbai in June |\\\\n| July | 29° / 27° | 26 | 5 | 0 | 1318 mm | Ok | Mumbai in July |\\\\n| August | 29° / 26° | 21 | 10 | 0 | 779 mm | Ok | Mumbai in August |\\\\n| September | 30° / 26° | 14 | 16 | 0 | 408 mm | Ok | Mumbai in September |\\\\n| October | 33° / 27° | 4 | 27 | 0 | 106 mm | Good | Mumbai in October |\\\\n| November | 34° / 26° | 2 | 28 | 0 | 44 mm | Ok | Mumbai in November |\\\\n| December | 32° / 24° | 0 | 31 | 0 | 8 mm | Good | Mumbai in December |\\', \\'score\\': 0.82694983}, {\\'title\\': \\'Mumbai, Maharashtra, India Monthly Weather\\', \\'url\\': \\'https://www.accuweather.com/en/in/mumbai/204842/june-weather/204842\\', \\'content\\': \\'# Mumbai, Maharashtra\\\\n\\\\nMumbai\\\\n\\\\nMaharashtra\\\\n\\\\n## Around the Globe\\\\n\\\\nAround the Globe\\\\n\\\\n### Hurricane Tracker\\\\n\\\\n### Severe Weather\\\\n\\\\n### Radar & Maps\\\\n\\\\n### News & Features\\\\n\\\\n### Astronomy\\\\n\\\\n### Business\\\\n\\\\n### Climate\\\\n\\\\n### Health\\\\n\\\\n### Recreation\\\\n\\\\n### Sports\\\\n\\\\n### Travel\\\\n\\\\n### Warnings\\\\n\\\\n### Data Suite\\\\n\\\\n### Forensics\\\\n\\\\n### Advertising\\\\n\\\\n### Superior Accuracy™\\\\n\\\\n### Video\\\\n\\\\n## Monthly\\\\n\\\\n## June\\\\n\\\\n## 2025\\\\n\\\\n## Daily\\\\n\\\\n## Temperature Graph\\\\n\\\\n## Further Ahead\\\\n\\\\nFurther Ahead\\\\n\\\\n### July 2025\\\\n\\\\n### August 2025 [...] ### September 2025\\\\n\\\\n## Around the Globe\\\\n\\\\nAround the Globe\\\\n\\\\n### Hurricane Tracker\\\\n\\\\n### Severe Weather\\\\n\\\\n### Radar & Maps\\\\n\\\\n### News\\\\n\\\\n### Video\\\\n\\\\nTop Stories\\\\n\\\\nHurricane\\\\n\\\\nAtlantic tropical rainstorm may reach Caribbean as a hurricane\\\\n\\\\n1 hour ago\\\\n\\\\nWeather News\\\\n\\\\nTexas to enact sweeping camp safety law to protect kids\\\\n\\\\n18 hours ago\\\\n\\\\nWeather Forecasts\\\\n\\\\nOctober-like conditions to spread across much of central, eastern U.S.\\\\n\\\\n1 hour ago\\\\n\\\\nAstronomy [...] 85% of world population may see total lunar eclipse on Sunday\\\\n\\\\n19 hours ago\\\\n\\\\nSevere Weather\\\\n\\\\nSevere storms to spring to life in eastern US\\\\n\\\\n10 hours ago\\\\n\\\\nFeatured Stories\\\\n\\\\nWeather News\\\\n\\\\nFAA investigating plane that went off end of runway after landing at C...\\\\n\\\\n1 day ago\\\\n\\\\nWinter Weather\\\\n\\\\nCan woolly bear caterpillars predict winter weather?\\\\n\\\\n16 hours ago\\\\n\\\\nRecreation\\\\n\\\\nPowerball win or a lightning strike: Which has better odds?\\\\n\\\\n2 days ago\\\\n\\\\nWeather News\\', \\'score\\': 0.4767995}, {\\'title\\': \\'Mumbai, Maharashtra, India Monthly Weather - AccuWeather\\', \\'url\\': \\'https://www.accuweather.com/en/in/mumbai/204842/september-weather/204842\\', \\'content\\': \\'Get the monthly weather forecast for Mumbai, Maharashtra, India, including daily high/low, historical averages, to help you plan ahead.\\', \\'score\\': 0.15452935}]', name='tavily_search_results_json', tool_call_id='33c921b1-61be-4736-86fd-2da090a33f2b')]\n",
      "\n",
      "2025-09-06 14:25:31 | DEBUG    | __main__ | 🧠 Processing messages through language model...\n",
      "2025-09-06 14:25:33 | DEBUG    | __main__ | 🔍 Model response: content=\"I couldn't find the weather for today in Mumbai, but I found the forecast for September 2025. It's expected to be very hot with average temperatures between 26°C and 30°C, and there's a high chance of rain for about half of the month.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--c14244dd-0809-44e1-abb1-97db609595b2-0' usage_metadata={'input_tokens': 1871, 'output_tokens': 231, 'total_tokens': 2102, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 166}}\n",
      "2025-09-06 14:25:33 | DEBUG    | __main__ | ✅ No tool calls detected, ending workflow.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "👤 User: do you remember my name?\n",
      "2025-09-06 14:25:55 | INFO     | __main__ | 👤 User (id: 285829db-1aaf-4470-85f6-902094fcfa66): do you remember my name?\n",
      "🤖 Assistant: 2025-09-06 14:25:55 | DEBUG    | __main__ | 🧠 Processing messages through language model...\n",
      "2025-09-06 14:25:56 | DEBUG    | __main__ | 🔍 Model response: content='Yes, your name is Jiten.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--03241744-a363-477d-971d-4dd468b3d8e3-0' usage_metadata={'input_tokens': 1944, 'output_tokens': 8, 'total_tokens': 1952, 'input_token_details': {'cache_read': 0}}\n",
      "2025-09-06 14:25:56 | DEBUG    | __main__ | ✅ No tool calls detected, ending workflow.\n",
      "\n",
      "------------------------------------------------------------\n",
      "👋 Goodbye!\n"
     ]
    }
   ],
   "source": [
    "await interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4181a1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InfoBuddy.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
